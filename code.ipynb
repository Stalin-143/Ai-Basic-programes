{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class LLMTrainingRecommender:\n",
    "    def __init__(self, dataset_dir=\"dataset\", sample_size=5000, min_token_freq=5):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.file_info = []\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.similarity_matrix = None\n",
    "        self.sample_size = sample_size\n",
    "        self.min_token_freq = min_token_freq\n",
    "        self.corpus_stats = {}\n",
    "        self.token_frequency = Counter()\n",
    "        self.file_quality_scores = {}\n",
    "        \n",
    "    def scan_datasets(self, max_workers=4):\n",
    "        \"\"\"Scan the dataset directory and collect information about all files.\"\"\"\n",
    "        logging.info(f\"Scanning {self.dataset_dir} for potential LLM training datasets...\")\n",
    "        \n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(self.dataset_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_files.append(file_path)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(self._process_file, all_files))\n",
    "            \n",
    "        # Filter out None results (files that couldn't be processed)\n",
    "        self.file_info = [result for result in results if result is not None]\n",
    "        \n",
    "        logging.info(f\"Found {len(self.file_info)} usable files for LLM training\")\n",
    "        return self.file_info\n",
    "    \n",
    "    def _process_file(self, file_path):\n",
    "        \"\"\"Process a single file and extract its information.\"\"\"\n",
    "        try:\n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            \n",
    "            # Only process text-based files that can be used for LLM training\n",
    "            if file_extension not in ['.txt', '.csv', '.json', '.dat', '.jsonl']:\n",
    "                return None\n",
    "                \n",
    "            # Extract metadata based on file type\n",
    "            metadata = self._extract_metadata(file_path, file_extension)\n",
    "            \n",
    "            # Compute MD5 hash for duplicate detection\n",
    "            file_hash = self._compute_file_hash(file_path)\n",
    "            \n",
    "            return {\n",
    "                'path': file_path,\n",
    "                'name': file_name,\n",
    "                'extension': file_extension,\n",
    "                'size': file_size,\n",
    "                'hash': file_hash,\n",
    "                'metadata': metadata\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_metadata(self, file_path, extension):\n",
    "        \"\"\"Extract metadata relevant to LLM training from files.\"\"\"\n",
    "        metadata = {\n",
    "            'row_count': None,\n",
    "            'token_count': None,\n",
    "            'avg_length': None,\n",
    "            'language': 'unknown',\n",
    "            'quality_score': None,\n",
    "            'content_sample': \"\",\n",
    "            'data_format': None,\n",
    "            'vocabulary_richness': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            text_sample = \"\"\n",
    "            \n",
    "            if extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    lines = f.readlines()\n",
    "                    metadata['row_count'] = len(lines)\n",
    "                    \n",
    "                    # Get a sample of the text for analysis\n",
    "                    sample_size = min(self.sample_size, len(lines))\n",
    "                    sample_indices = random.sample(range(len(lines)), sample_size) if len(lines) > sample_size else range(len(lines))\n",
    "                    text_sample = \"\".join([lines[i] for i in sample_indices])\n",
    "                    metadata['data_format'] = 'plain_text'\n",
    "                    \n",
    "            elif extension == '.csv':\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    csv_reader = csv.reader(f)\n",
    "                    headers = next(csv_reader, [])\n",
    "                    rows = list(csv_reader)\n",
    "                    metadata['row_count'] = len(rows)\n",
    "                    metadata['data_format'] = 'tabular'\n",
    "                    \n",
    "                    # Look for text columns that could be used for training\n",
    "                    text_columns = []\n",
    "                    for i, header in enumerate(headers):\n",
    "                        if header.lower() in ['text', 'content', 'description', 'prompt', 'response', 'message']:\n",
    "                            text_columns.append(i)\n",
    "                    \n",
    "                    # Get a sample of text from identified columns\n",
    "                    if text_columns:\n",
    "                        sample_size = min(self.sample_size, len(rows))\n",
    "                        sample_indices = random.sample(range(len(rows)), sample_size) if len(rows) > sample_size else range(len(rows))\n",
    "                        for idx in sample_indices:\n",
    "                            for col in text_columns:\n",
    "                                if col < len(rows[idx]):\n",
    "                                    text_sample += rows[idx][col] + \" \"\n",
    "                    \n",
    "            elif extension in ['.json', '.jsonl']:\n",
    "                is_jsonl = extension == '.jsonl'\n",
    "                text_fields = []\n",
    "                \n",
    "                if is_jsonl:\n",
    "                    # For JSONL, read line by line\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        lines = f.readlines()\n",
    "                        metadata['row_count'] = len(lines)\n",
    "                        \n",
    "                        # Sample lines\n",
    "                        sample_size = min(self.sample_size, len(lines))\n",
    "                        sample_indices = random.sample(range(len(lines)), sample_size) if len(lines) > sample_size else range(len(lines))\n",
    "                        \n",
    "                        for idx in sample_indices:\n",
    "                            try:\n",
    "                                obj = json.loads(lines[idx])\n",
    "                                # Extract text fields recursively\n",
    "                                self._extract_text_fields(obj, \"\", text_fields)\n",
    "                            except:\n",
    "                                pass\n",
    "                else:\n",
    "                    # Regular JSON\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        data = json.load(f)\n",
    "                        if isinstance(data, list):\n",
    "                            metadata['row_count'] = len(data)\n",
    "                            # Sample entries\n",
    "                            sample_size = min(self.sample_size, len(data))\n",
    "                            sample_indices = random.sample(range(len(data)), sample_size) if len(data) > sample_size else range(len(data))\n",
    "                            \n",
    "                            for idx in sample_indices:\n",
    "                                self._extract_text_fields(data[idx], \"\", text_fields)\n",
    "                        else:\n",
    "                            # Single object\n",
    "                            metadata['row_count'] = 1\n",
    "                            self._extract_text_fields(data, \"\", text_fields)\n",
    "                \n",
    "                text_sample = \" \".join([field[1] for field in text_fields if isinstance(field[1], str)])\n",
    "                metadata['data_format'] = 'json'\n",
    "                            \n",
    "            elif extension == '.dat':\n",
    "                # Try different approaches to read the file\n",
    "                try:\n",
    "                    # Try as pickle\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                        if isinstance(data, list) or isinstance(data, dict):\n",
    "                            metadata['data_format'] = 'pickle'\n",
    "                            # Extract strings if possible\n",
    "                            if isinstance(data, list):\n",
    "                                text_items = [item for item in data if isinstance(item, str)]\n",
    "                                text_sample = \" \".join(text_items[:self.sample_size])\n",
    "                except:\n",
    "                    # Try as text\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            lines = f.readlines()\n",
    "                            metadata['row_count'] = len(lines)\n",
    "                            \n",
    "                            # Get a sample of the text\n",
    "                            sample_size = min(self.sample_size, len(lines))\n",
    "                            sample_indices = random.sample(range(len(lines)), sample_size) if len(lines) > sample_size else range(len(lines))\n",
    "                            text_sample = \"\".join([lines[i] for i in sample_indices])\n",
    "                            metadata['data_format'] = 'unknown_text'\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Analyze the text sample\n",
    "            if text_sample:\n",
    "                # Simple tokenization\n",
    "                tokens = re.findall(r'\\b\\w+\\b', text_sample.lower())\n",
    "                metadata['token_count'] = len(tokens)\n",
    "                metadata['avg_length'] = len(text_sample) / max(1, metadata['row_count'] if metadata['row_count'] else 1)\n",
    "                \n",
    "                # Vocabulary richness (unique words / total words)\n",
    "                unique_tokens = set(tokens)\n",
    "                metadata['vocabulary_richness'] = len(unique_tokens) / max(1, len(tokens))\n",
    "                \n",
    "                # Update global token frequency\n",
    "                self.token_frequency.update(tokens)\n",
    "                \n",
    "                # Store a small content sample\n",
    "                metadata['content_sample'] = text_sample[:500]\n",
    "                \n",
    "                # Guess language based on common words\n",
    "                metadata['language'] = self._detect_language(tokens)\n",
    "                \n",
    "                # Calculate quality score\n",
    "                metadata['quality_score'] = self._calculate_quality_score(tokens, text_sample)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error extracting metadata from {file_path}: {str(e)}\")\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    def _extract_text_fields(self, obj, prefix, result):\n",
    "        \"\"\"Recursively extract text fields from a JSON object.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "                \n",
    "                if isinstance(value, str) and len(value) > 10:\n",
    "                    result.append((new_prefix, value))\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    self._extract_text_fields(value, new_prefix, result)\n",
    "        elif isinstance(obj, list):\n",
    "            for i, item in enumerate(obj):\n",
    "                new_prefix = f\"{prefix}[{i}]\"\n",
    "                self._extract_text_fields(item, new_prefix, result)\n",
    "    \n",
    "    def _compute_file_hash(self, file_path):\n",
    "        \"\"\"Compute MD5 hash of a file.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def _detect_language(self, tokens):\n",
    "        \"\"\"Simple language detection based on common words.\"\"\"\n",
    "        # Count common words in different languages\n",
    "        english_common = set(['the', 'and', 'of', 'to', 'a', 'in', 'that', 'is', 'was', 'it'])\n",
    "        spanish_common = set(['el', 'la', 'de', 'que', 'y', 'en', 'un', 'ser', 'se', 'no'])\n",
    "        french_common = set(['le', 'la', 'de', 'et', 'Ã ', 'en', 'un', 'est', 'que', 'dans'])\n",
    "        \n",
    "        # Count occurrences\n",
    "        en_count = sum(1 for token in tokens if token in english_common)\n",
    "        es_count = sum(1 for token in tokens if token in spanish_common)\n",
    "        fr_count = sum(1 for token in tokens if token in french_common)\n",
    "        \n",
    "        # Determine language\n",
    "        if en_count > es_count and en_count > fr_count:\n",
    "            return 'english'\n",
    "        elif es_count > en_count and es_count > fr_count:\n",
    "            return 'spanish'\n",
    "        elif fr_count > en_count and fr_count > es_count:\n",
    "            return 'french'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    def _calculate_quality_score(self, tokens, text_sample):\n",
    "        \"\"\"Calculate a quality score for LLM training data.\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Length score - longer texts are usually better for training\n",
    "        length_score = min(1.0, len(tokens) / 1000) * 2\n",
    "        score += length_score\n",
    "        \n",
    "        # Vocabulary richness score\n",
    "        unique_tokens = set(tokens)\n",
    "        vocab_score = min(1.0, len(unique_tokens) / max(1, len(tokens)) * 5)\n",
    "        score += vocab_score\n",
    "        \n",
    "        # Sentence structure score (look for punctuation)\n",
    "        punctuation_count = text_sample.count('.') + text_sample.count('!') + text_sample.count('?')\n",
    "        sentence_score = min(1.0, punctuation_count / max(1, len(text_sample) / 100))\n",
    "        score += sentence_score\n",
    "        \n",
    "        # Check for formatting issues\n",
    "        formatting_score = 1.0\n",
    "        if text_sample.count('\\n\\n\\n') > 5:  # Too many line breaks\n",
    "            formatting_score -= 0.5\n",
    "        if re.search(r'[A-Z]{10,}', text_sample):  # All caps sections\n",
    "            formatting_score -= 0.3\n",
    "        score += formatting_score\n",
    "        \n",
    "        # Normalize to 0-10 scale\n",
    "        return min(10, max(0, score * 2))\n",
    "    \n",
    "    def analyze_corpus(self):\n",
    "        \"\"\"Analyze the entire corpus for LLM training suitability.\"\"\"\n",
    "        if not self.file_info:\n",
    "            logging.warning(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "            return {}\n",
    "        \n",
    "        total_tokens = sum(file['metadata']['token_count'] for file in self.file_info if file['metadata']['token_count'])\n",
    "        total_files = len(self.file_info)\n",
    "        \n",
    "        # Quality distribution\n",
    "        quality_scores = [file['metadata']['quality_score'] for file in self.file_info if file['metadata']['quality_score']]\n",
    "        avg_quality = sum(quality_scores) / max(1, len(quality_scores))\n",
    "        \n",
    "        # Language distribution\n",
    "        languages = [file['metadata']['language'] for file in self.file_info]\n",
    "        language_counts = Counter(languages)\n",
    "        \n",
    "        # Format distribution\n",
    "        formats = [file['metadata']['data_format'] for file in self.file_info if file['metadata']['data_format']]\n",
    "        format_counts = Counter(formats)\n",
    "        \n",
    "        # Vocabulary analysis\n",
    "        rare_tokens = {word: count for word, count in self.token_frequency.items() if count <= self.min_token_freq}\n",
    "        common_tokens = {word: count for word, count in self.token_frequency.most_common(100)}\n",
    "        \n",
    "        # Create duplicate groups based on file hash\n",
    "        hash_to_files = {}\n",
    "        for file in self.file_info:\n",
    "            if file['hash'] in hash_to_files:\n",
    "                hash_to_files[file['hash']].append(file['path'])\n",
    "            else:\n",
    "                hash_to_files[file['hash']] = [file['path']]\n",
    "        \n",
    "        duplicate_groups = {hash_val: files for hash_val, files in hash_to_files.items() if len(files) > 1}\n",
    "        \n",
    "        self.corpus_stats = {\n",
    "            'total_files': total_files,\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_quality_score': avg_quality,\n",
    "            'language_distribution': language_counts,\n",
    "            'format_distribution': format_counts,\n",
    "            'rare_token_count': len(rare_tokens),\n",
    "            'duplicate_groups': duplicate_groups\n",
    "        }\n",
    "        \n",
    "        return self.corpus_stats\n",
    "    \n",
    "    def build_similarity_matrix(self):\n",
    "        \"\"\"Build a similarity matrix based on file metadata and content.\"\"\"\n",
    "        if not self.file_info:\n",
    "            logging.warning(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "            return\n",
    "        \n",
    "        # Create document descriptions for each file\n",
    "        documents = []\n",
    "        for file in self.file_info:\n",
    "            meta = file['metadata']\n",
    "            \n",
    "            # Base document on the content sample and metadata\n",
    "            doc = f\"{file['name']} \"\n",
    "            \n",
    "            if meta['content_sample']:\n",
    "                doc += meta['content_sample']\n",
    "            \n",
    "            if meta['data_format']:\n",
    "                doc += f\" format:{meta['data_format']}\"\n",
    "                \n",
    "            if meta['language']:\n",
    "                doc += f\" language:{meta['language']}\"\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        if not documents:\n",
    "            logging.warning(\"No document content available for similarity analysis.\")\n",
    "            return\n",
    "        \n",
    "        # Create TF-IDF matrix\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            self.similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "            \n",
    "            logging.info(\"Similarity matrix built successfully\")\n",
    "            return self.similarity_matrix\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error building similarity matrix: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_recommendations(self, file_path=None, query=None, criteria=None, top_n=5):\n",
    "        \"\"\"Get recommendations for LLM training based on file or query.\"\"\"\n",
    "        if self.similarity_matrix is None:\n",
    "            self.build_similarity_matrix()\n",
    "            \n",
    "        if not self.similarity_matrix is not None:\n",
    "            logging.warning(\"Similarity matrix is not available. Cannot provide recommendations.\")\n",
    "            return []\n",
    "        \n",
    "        # Define the criteria if not provided\n",
    "        if criteria is None:\n",
    "            criteria = {\n",
    "                'quality_min': 5.0,  # Minimum quality score\n",
    "                'token_min': 100,    # Minimum token count\n",
    "                'prefer_language': None  # Preferred language\n",
    "            }\n",
    "            \n",
    "        result = []\n",
    "        \n",
    "        if file_path:\n",
    "            # Find the index of the file\n",
    "            file_index = None\n",
    "            for i, file in enumerate(self.file_info):\n",
    "                if file['path'] == file_path:\n",
    "                    file_index = i\n",
    "                    break\n",
    "            \n",
    "            if file_index is None:\n",
    "                logging.warning(f\"File {file_path} not found in the dataset directory.\")\n",
    "                return []\n",
    "            \n",
    "            # Get similarity scores for this file\n",
    "            similarity_scores = list(enumerate(self.similarity_matrix[file_index]))\n",
    "            \n",
    "            # Sort by similarity (excluding the file itself)\n",
    "            similarity_scores = sorted(\n",
    "                [score for score in similarity_scores if score[0] != file_index],\n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Apply filtering based on criteria\n",
    "            filtered_scores = []\n",
    "            for idx, score in similarity_scores:\n",
    "                file = self.file_info[idx]\n",
    "                meta = file['metadata']\n",
    "                \n",
    "                if (meta['quality_score'] is None or meta['quality_score'] >= criteria['quality_min']) and \\\n",
    "                   (meta['token_count'] is None or meta['token_count'] >= criteria['token_min']) and \\\n",
    "                   (criteria['prefer_language'] is None or meta['language'] == criteria['prefer_language']):\n",
    "                    filtered_scores.append((idx, score))\n",
    "            \n",
    "            # Get top N recommendations\n",
    "            top_recommendations = filtered_scores[:top_n]\n",
    "            \n",
    "            for idx, score in top_recommendations:\n",
    "                result.append({\n",
    "                    'file': self.file_info[idx],\n",
    "                    'similarity_score': score\n",
    "                })\n",
    "                \n",
    "        elif query:\n",
    "            # Transform query using the same vectorizer\n",
    "            query_vector = self.vectorizer.transform([query])\n",
    "            \n",
    "            # Get similarity scores for all files\n",
    "            similarity_scores = []\n",
    "            for i, file in enumerate(self.file_info):\n",
    "                if self.similarity_matrix is not None and i < len(self.similarity_matrix):\n",
    "                    sim_score = cosine_similarity(query_vector, self.vectorizer.transform([file['metadata']['content_sample']]))[0][0]\n",
    "                    similarity_scores.append((i, sim_score))\n",
    "            \n",
    "            # Sort by similarity\n",
    "            similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Apply filtering based on criteria\n",
    "            filtered_scores = []\n",
    "            for idx, score in similarity_scores:\n",
    "                file = self.file_info[idx]\n",
    "                meta = file['metadata']\n",
    "                \n",
    "                if (meta['quality_score'] is None or meta['quality_score'] >= criteria['quality_min']) and \\\n",
    "                   (meta['token_count'] is None or meta['token_count'] >= criteria['token_min']) and \\\n",
    "                   (criteria['prefer_language'] is None or meta['language'] == criteria['prefer_language']):\n",
    "                    filtered_scores.append((idx, score))\n",
    "            \n",
    "            # Get top N recommendations\n",
    "            top_recommendations = filtered_scores[:top_n]\n",
    "            \n",
    "            for idx, score in top_recommendations:\n",
    "                result.append({\n",
    "                    'file': self.file_info[idx],\n",
    "                    'similarity_score': score\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_training_mix_recommendation(self, target_tokens=1000000, quality_threshold=5.0):\n",
    "        \"\"\"Recommend a mix of files for LLM training to reach a target token count.\"\"\"\n",
    "        if not self.file_info:\n",
    "            logging.warning(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "            return []\n",
    "            \n",
    "        # Filter files by quality\n",
    "        quality_files = [file for file in self.file_info \n",
    "                         if file['metadata']['quality_score'] is not None \n",
    "                         and file['metadata']['quality_score'] >= quality_threshold\n",
    "                         and file['metadata']['token_count'] is not None]\n",
    "        \n",
    "        if not quality_files:\n",
    "            logging.warning(f\"No files meet the quality threshold of {quality_threshold}\")\n",
    "            return []\n",
    "            \n",
    "        # Sort by quality score (descending)\n",
    "        sorted_files = sorted(quality_files, key=lambda x: x['metadata']['quality_score'], reverse=True)\n",
    "        \n",
    "        # Calculate cumulative tokens\n",
    "        selected_files = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for file in sorted_files:\n",
    "            if current_tokens >= target_tokens:\n",
    "                break\n",
    "                \n",
    "            selected_files.append({\n",
    "                'file': file,\n",
    "                'tokens': file['metadata']['token_count'],\n",
    "                'quality': file['metadata']['quality_score']\n",
    "            })\n",
    "            \n",
    "            current_tokens += file['metadata']['token_count']\n",
    "        \n",
    "        return {\n",
    "            'selected_files': selected_files,\n",
    "            'total_tokens': current_tokens,\n",
    "            'target_tokens': target_tokens,\n",
    "            'coverage': min(1.0, current_tokens / target_tokens)\n",
    "        }\n",
    "    \n",
    "    def visualize_corpus_stats(self):\n",
    "        \"\"\"Visualize statistics about the corpus.\"\"\"\n",
    "        if not self.corpus_stats:\n",
    "            self.analyze_corpus()\n",
    "            \n",
    "        if not self.corpus_stats:\n",
    "            logging.warning(\"No corpus statistics available.\")\n",
    "            return\n",
    "            \n",
    "        # Create a figure with multiple subplots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Quality score distribution\n",
    "        quality_scores = [file['metadata']['quality_score'] for file in self.file_info if file['metadata']['quality_score'] is not None]\n",
    "        axs[0, 0].hist(quality_scores, bins=10, color='skyblue', edgecolor='black')\n",
    "        axs[0, 0].set_title('Quality Score Distribution')\n",
    "        axs[0, 0].set_xlabel('Quality Score (0-10)')\n",
    "        axs[0, 0].set_ylabel('Number of Files')\n",
    "        \n",
    "        # 2. Language distribution\n",
    "        if self.corpus_stats['language_distribution']:\n",
    "            languages = list(self.corpus_stats['language_distribution'].keys())\n",
    "            counts = list(self.corpus_stats['language_distribution'].values())\n",
    "            axs[0, 1].bar(languages, counts, color='lightgreen')\n",
    "            axs[0, 1].set_title('Language Distribution')\n",
    "            axs[0, 1].set_xlabel('Language')\n",
    "            axs[0, 1].set_ylabel('Number of Files')\n",
    "            axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. File format distribution\n",
    "        if self.corpus_stats['format_distribution']:\n",
    "            formats = list(self.corpus_stats['format_distribution'].keys())\n",
    "            counts = list(self.corpus_stats['format_distribution'].values())\n",
    "            axs[1, 0].bar(formats, counts, color='salmon')\n",
    "            axs[1, 0].set_title('File Format Distribution')\n",
    "            axs[1, 0].set_xlabel('Format')\n",
    "            axs[1, 0].set_ylabel('Number of Files')\n",
    "            axs[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 4. Token count distribution\n",
    "        token_counts = [file['metadata']['token_count'] for file in self.file_info if file['metadata']['token_count'] is not None]\n",
    "        if token_counts:\n",
    "            axs[1, 1].hist(token_counts, bins=20, color='plum', edgecolor='black')\n",
    "            axs[1, 1].set_title('Token Count Distribution')\n",
    "            axs[1, 1].set_xlabel('Number of Tokens')\n",
    "            axs[1, 1].set_ylabel('Number of Files')\n",
    "            axs[1, 1].set_xscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the recommender\n",
    "    recommender = LLMTrainingRecommender(dataset_dir=\"dataset\")\n",
    "    \n",
    "    # Scan datasets\n",
    "    recommender.scan_datasets()\n",
    "    \n",
    "    # Analyze corpus\n",
    "    corpus_stats = recommender.analyze_corpus()\n",
    "    print(\"\\nCorpus Statistics:\")\n",
    "    print(f\"Total files: {corpus_stats['total_files']}\")\n",
    "    print(f\"Total tokens: {corpus_stats['total_tokens']:,}\")\n",
    "    print(f\"Average quality score: {corpus_stats['avg_quality_score']:.2f}/10\")\n",
    "    \n",
    "    # Build similarity matrix\n",
    "    recommender.build_similarity_matrix()\n",
    "    \n",
    "    # Visualize corpus statistics\n",
    "    recommender.visualize_corpus_stats()\n",
    "    \n",
    "    # Get training mix recommendation\n",
    "    print(\"\\nRecommended Training Mix:\")\n",
    "    training_mix = recommender.get_training_mix_recommendation(target_tokens=1000000)\n",
    "    \n",
    "    if training_mix['selected_files']:\n",
    "        print(f\"Selected {len(training_mix['selected_files'])} files\")\n",
    "        print(f\"Total tokens: {training_mix['total_tokens']:,} ({training_mix['coverage']*100:.1f}% of target)\")\n",
    "        \n",
    "        print(\"\\nTop 5 recommended files for training:\")\n",
    "        for i, item in enumerate(training_mix['selected_files'][:5], 1):\n",
    "            file = item['file']\n",
    "            print(f\"{i}. {file['name']}\")\n",
    "            print(f\"   Quality: {item['quality']:.1f}/10, Tokens: {item['tokens']:,}\")\n",
    "            print(f\"   Language: {file['metadata']['language']}, Format: {file['metadata']['data_format']}\")\n",
    "            print(f\"   Path: {file['path']}\")\n",
    "            if file['metadata']['content_sample']:\n",
    "                print(f\"   Sample: {file['metadata']['content_sample'][:100]}...\")\n",
    "            print()\n",
    "    \n",
    "    # Example: Get similar files for a specific file\n",
    "    if recommender.file_info:\n",
    "        example_file = recommender.file_info[0]['path']\n",
    "        print(f\"\\nSimilar files to {os.path.basename(example_file)}:\")\n",
    "        similar_files = recommender.get_recommendations(file_path=example_file, top_n=3)\n",
    "        \n",
    "        for i, rec in enumerate(similar_files, 1):\n",
    "            file = rec['file']\n",
    "            score = rec['similarity_score']\n",
    "            print(f\"{i}. {file['name']} (Score: {score:.2f})\")\n",
    "            print(f\"   Quality: {file['metadata']['quality_score']:.1f}/10, Tokens: {file['metadata']['token_count']:,}\")\n",
    "            print(f\"   Language: {file['metadata']['language']}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "recommender = LLMTrainingRecommender(dataset_dir=\"dataset\")\n",
    "recommender.scan_datasets()\n",
    "corpus_stats = recommender.analyze_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/lib/python3.13/site-packages/airdrop_ng-1.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/lib/python3.13/site-packages/airgraph_ng-1.1-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.13/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3.13/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3.13/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3.13/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata\n",
      "Successfully installed tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --break-system-packages scikit-learn pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning dataset for datasets...\n",
      "Found 32 files in the dataset directory.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for dataset/Unconfirmed 94663.crdownload:\n",
      "1. ua.base (Score: 0.03)\n",
      "   Type: .base, Size: 1792501 bytes\n",
      "   Path: dataset/ml-100k/ua.base\n",
      "   Description: File with .base extension\n",
      "\n",
      "2. u5.test (Score: 0.03)\n",
      "   Type: .test, Size: 397397 bytes\n",
      "   Path: dataset/ml-100k/u5.test\n",
      "   Description: File with .test extension\n",
      "\n",
      "3. ua.test (Score: 0.03)\n",
      "   Type: .test, Size: 186672 bytes\n",
      "   Path: dataset/ml-100k/ua.test\n",
      "   Description: File with .test extension\n",
      "\n",
      "4. u3.base (Score: 0.03)\n",
      "   Type: .base, Size: 1582546 bytes\n",
      "   Path: dataset/ml-100k/u3.base\n",
      "   Description: File with .base extension\n",
      "\n",
      "5. u4.base (Score: 0.03)\n",
      "   Type: .base, Size: 1581878 bytes\n",
      "   Path: dataset/ml-100k/u4.base\n",
      "   Description: File with .base extension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "class DatasetRecommender:\n",
    "    def __init__(self, dataset_dir=\"dataset\"):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.file_info = []\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.similarity_matrix = None\n",
    "        \n",
    "    def scan_datasets(self):\n",
    "        \"\"\"Scan the dataset directory and collect information about all files.\"\"\"\n",
    "        print(f\"Scanning {self.dataset_dir} for datasets...\")\n",
    "        \n",
    "        for root, _, files in os.walk(self.dataset_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_extension = os.path.splitext(file)[1].lower()\n",
    "                \n",
    "                # Get file size\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                \n",
    "                # Extract metadata based on file type\n",
    "                metadata = self._extract_metadata(file_path, file_extension)\n",
    "                \n",
    "                self.file_info.append({\n",
    "                    'path': file_path,\n",
    "                    'name': file,\n",
    "                    'extension': file_extension,\n",
    "                    'size': file_size,\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "        \n",
    "        print(f\"Found {len(self.file_info)} files in the dataset directory.\")\n",
    "        return self.file_info\n",
    "    \n",
    "    def _extract_metadata(self, file_path, extension):\n",
    "        \"\"\"Extract metadata from files based on their type.\"\"\"\n",
    "        metadata = {\n",
    "            'row_count': None,\n",
    "            'column_count': None,\n",
    "            'columns': [],\n",
    "            'dimensions': None,\n",
    "            'description': \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if extension in ['.csv', '.txt']:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    csv_reader = csv.reader(f)\n",
    "                    headers = next(csv_reader, [])\n",
    "                    metadata['columns'] = headers\n",
    "                    metadata['column_count'] = len(headers)\n",
    "                    \n",
    "                    # Count rows\n",
    "                    row_count = sum(1 for _ in csv_reader)\n",
    "                    metadata['row_count'] = row_count\n",
    "                    \n",
    "                    metadata['description'] = f\"CSV file with {row_count} rows and {len(headers)} columns\"\n",
    "                    \n",
    "            elif extension in ['.xlsx', '.xls']:\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    metadata['columns'] = df.columns.tolist()\n",
    "                    metadata['column_count'] = len(df.columns)\n",
    "                    metadata['row_count'] = len(df)\n",
    "                    metadata['description'] = f\"Excel file with {len(df)} rows and {len(df.columns)} columns\"\n",
    "                except Exception as e:\n",
    "                    metadata['description'] = f\"Excel file (error reading: {str(e)})\"\n",
    "                    \n",
    "            elif extension in ['.png', '.jpg', '.jpeg']:\n",
    "                try:\n",
    "                    img = Image.open(file_path)\n",
    "                    metadata['dimensions'] = f\"{img.width}x{img.height}\"\n",
    "                    metadata['description'] = f\"Image file ({metadata['dimensions']})\"\n",
    "                except Exception as e:\n",
    "                    metadata['description'] = f\"Image file (error reading: {str(e)})\"\n",
    "                    \n",
    "            elif extension == '.json':\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        data = json.load(f)\n",
    "                    if isinstance(data, list):\n",
    "                        metadata['row_count'] = len(data)\n",
    "                        metadata['description'] = f\"JSON file with {len(data)} records\"\n",
    "                    else:\n",
    "                        metadata['description'] = \"JSON file with object structure\"\n",
    "                except Exception as e:\n",
    "                    metadata['description'] = f\"JSON file (error reading: {str(e)})\"\n",
    "                    \n",
    "            elif extension == '.dat':\n",
    "                try:\n",
    "                    # Try to read as pickle first\n",
    "                    try:\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            data = pickle.load(f)\n",
    "                        metadata['description'] = f\"DAT file (pickle format)\"\n",
    "                    except:\n",
    "                        # Try to read as text\n",
    "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            lines = list(f)\n",
    "                        metadata['row_count'] = len(lines)\n",
    "                        metadata['description'] = f\"DAT file with {len(lines)} lines\"\n",
    "                except Exception as e:\n",
    "                    metadata['description'] = f\"DAT file (binary or text)\"\n",
    "            \n",
    "            else:\n",
    "                # For other file types\n",
    "                metadata['description'] = f\"File with {extension} extension\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            metadata['description'] = f\"Error analyzing file: {str(e)}\"\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    def build_similarity_matrix(self):\n",
    "        \"\"\"Build a similarity matrix based on file metadata.\"\"\"\n",
    "        if not self.file_info:\n",
    "            print(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "            return\n",
    "        \n",
    "        # Create document descriptions for each file\n",
    "        documents = []\n",
    "        for file in self.file_info:\n",
    "            doc = f\"{file['name']} {file['extension']} \"\n",
    "            \n",
    "            # Add metadata information\n",
    "            meta = file['metadata']\n",
    "            if meta['columns']:\n",
    "                doc += \" \".join(meta['columns']) + \" \"\n",
    "            if meta['description']:\n",
    "                doc += meta['description']\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Create TF-IDF matrix\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        self.similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        return self.similarity_matrix\n",
    "    \n",
    "    def get_recommendations(self, file_path, top_n=5):\n",
    "        \"\"\"Get recommendations for a given file.\"\"\"\n",
    "        if self.similarity_matrix is None:\n",
    "            self.build_similarity_matrix()\n",
    "            \n",
    "        # Find the index of the file\n",
    "        file_index = None\n",
    "        for i, file in enumerate(self.file_info):\n",
    "            if file['path'] == file_path:\n",
    "                file_index = i\n",
    "                break\n",
    "        \n",
    "        if file_index is None:\n",
    "            print(f\"File {file_path} not found in the dataset directory.\")\n",
    "            return []\n",
    "        \n",
    "        # Get similarity scores for this file\n",
    "        similarity_scores = list(enumerate(self.similarity_matrix[file_index]))\n",
    "        \n",
    "        # Sort by similarity (excluding the file itself)\n",
    "        similarity_scores = sorted(\n",
    "            [score for score in similarity_scores if score[0] != file_index],\n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_recommendations = similarity_scores[:top_n]\n",
    "        \n",
    "        result = []\n",
    "        for idx, score in top_recommendations:\n",
    "            result.append({\n",
    "                'file': self.file_info[idx],\n",
    "                'similarity_score': score\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_file_types(self):\n",
    "        \"\"\"Visualize the distribution of file types.\"\"\"\n",
    "        if not self.file_info:\n",
    "            print(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "            return\n",
    "        \n",
    "        # Count occurrences of each file extension\n",
    "        extension_counts = {}\n",
    "        for file in self.file_info:\n",
    "            ext = file['extension']\n",
    "            if ext in extension_counts:\n",
    "                extension_counts[ext] += 1\n",
    "            else:\n",
    "                extension_counts[ext] = 1\n",
    "        \n",
    "        # Create a bar chart\n",
    "        extensions = list(extension_counts.keys())\n",
    "        counts = list(extension_counts.values())\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(extensions, counts, color='skyblue')\n",
    "        plt.title('Distribution of File Types')\n",
    "        plt.xlabel('File Extension')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return extension_counts\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the recommender\n",
    "    recommender = DatasetRecommender(dataset_dir=\"dataset\")\n",
    "    \n",
    "    # Scan datasets\n",
    "    recommender.scan_datasets()\n",
    "    \n",
    "    # Build similarity matrix\n",
    "    recommender.build_similarity_matrix()\n",
    "    \n",
    "    # Display file type distribution\n",
    "    recommender.visualize_file_types()\n",
    "    \n",
    "    # Example: Get recommendations for a specific file\n",
    "    # Replace with an actual file path from your dataset directory\n",
    "    example_file = recommender.file_info[0]['path'] if recommender.file_info else None\n",
    "    \n",
    "    if example_file:\n",
    "        print(f\"\\nRecommendations for {example_file}:\")\n",
    "        recommendations = recommender.get_recommendations(example_file, top_n=5)\n",
    "        \n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            file = rec['file']\n",
    "            score = rec['similarity_score']\n",
    "            print(f\"{i}. {file['name']} (Score: {score:.2f})\")\n",
    "            print(f\"   Type: {file['extension']}, Size: {file['size']} bytes\")\n",
    "            print(f\"   Path: {file['path']}\")\n",
    "            print(f\"   Description: {file['metadata']['description']}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning dataset for datasets...\n",
      "Found 35 files in the dataset directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'dataset/news.tsv',\n",
       "  'name': 'news.tsv',\n",
       "  'extension': '.tsv',\n",
       "  'size': 84881998,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .tsv extension'}},\n",
       " {'path': 'dataset/entity_embedding.vec',\n",
       "  'name': 'entity_embedding.vec',\n",
       "  'extension': '.vec',\n",
       "  'size': 40305151,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .vec extension'}},\n",
       " {'path': 'dataset/relation_embedding.vec',\n",
       "  'name': 'relation_embedding.vec',\n",
       "  'extension': '.vec',\n",
       "  'size': 1044588,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .vec extension'}},\n",
       " {'path': 'dataset/behaviors.tsv',\n",
       "  'name': 'behaviors.tsv',\n",
       "  'extension': '.tsv',\n",
       "  'size': 1373844151,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .tsv extension'}},\n",
       " {'path': 'dataset/ml-100k/ua.base',\n",
       "  'name': 'ua.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1792501,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/u5.test',\n",
       "  'name': 'u5.test',\n",
       "  'extension': '.test',\n",
       "  'size': 397397,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-100k/u3.base',\n",
       "  'name': 'u3.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1582546,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/u.data',\n",
       "  'name': 'u.data',\n",
       "  'extension': '.data',\n",
       "  'size': 1979173,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .data extension'}},\n",
       " {'path': 'dataset/ml-100k/u4.base',\n",
       "  'name': 'u4.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1581878,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/u2.base',\n",
       "  'name': 'u2.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1583948,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/ub.test',\n",
       "  'name': 'ub.test',\n",
       "  'extension': '.test',\n",
       "  'size': 186697,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-100k/u5.base',\n",
       "  'name': 'u5.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1581776,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/u1.base',\n",
       "  'name': 'u1.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1586544,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/u.occupation',\n",
       "  'name': 'u.occupation',\n",
       "  'extension': '.occupation',\n",
       "  'size': 193,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .occupation extension'}},\n",
       " {'path': 'dataset/ml-100k/allbut.pl',\n",
       "  'name': 'allbut.pl',\n",
       "  'extension': '.pl',\n",
       "  'size': 716,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .pl extension'}},\n",
       " {'path': 'dataset/ml-100k/u2.test',\n",
       "  'name': 'u2.test',\n",
       "  'extension': '.test',\n",
       "  'size': 395225,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-100k/u.genre',\n",
       "  'name': 'u.genre',\n",
       "  'extension': '.genre',\n",
       "  'size': 202,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .genre extension'}},\n",
       " {'path': 'dataset/ml-100k/mku.sh',\n",
       "  'name': 'mku.sh',\n",
       "  'extension': '.sh',\n",
       "  'size': 643,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .sh extension'}},\n",
       " {'path': 'dataset/ml-100k/u.user',\n",
       "  'name': 'u.user',\n",
       "  'extension': '.user',\n",
       "  'size': 22628,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .user extension'}},\n",
       " {'path': 'dataset/ml-100k/u3.test',\n",
       "  'name': 'u3.test',\n",
       "  'extension': '.test',\n",
       "  'size': 396627,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-100k/u.info',\n",
       "  'name': 'u.info',\n",
       "  'extension': '.info',\n",
       "  'size': 36,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .info extension'}},\n",
       " {'path': 'dataset/ml-100k/u.item',\n",
       "  'name': 'u.item',\n",
       "  'extension': '.item',\n",
       "  'size': 236344,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .item extension'}},\n",
       " {'path': 'dataset/ml-100k/ua.test',\n",
       "  'name': 'ua.test',\n",
       "  'extension': '.test',\n",
       "  'size': 186672,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-100k/u4.test',\n",
       "  'name': 'u4.test',\n",
       "  'extension': '.test',\n",
       "  'size': 397295,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-100k/ub.base',\n",
       "  'name': 'ub.base',\n",
       "  'extension': '.base',\n",
       "  'size': 1792476,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .base extension'}},\n",
       " {'path': 'dataset/ml-100k/u1.test',\n",
       "  'name': 'u1.test',\n",
       "  'extension': '.test',\n",
       "  'size': 392629,\n",
       "  'metadata': {'row_count': None,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'File with .test extension'}},\n",
       " {'path': 'dataset/ml-20m/genome-scores.csv',\n",
       "  'name': 'genome-scores.csv',\n",
       "  'extension': '.csv',\n",
       "  'size': 323544381,\n",
       "  'metadata': {'row_count': 11709768,\n",
       "   'column_count': 3,\n",
       "   'columns': ['movieId', 'tagId', 'relevance'],\n",
       "   'dimensions': None,\n",
       "   'description': 'CSV file with 11709768 rows and 3 columns'}},\n",
       " {'path': 'dataset/ml-20m/movies.csv',\n",
       "  'name': 'movies.csv',\n",
       "  'extension': '.csv',\n",
       "  'size': 1397542,\n",
       "  'metadata': {'row_count': 27278,\n",
       "   'column_count': 3,\n",
       "   'columns': ['movieId', 'title', 'genres'],\n",
       "   'dimensions': None,\n",
       "   'description': 'CSV file with 27278 rows and 3 columns'}},\n",
       " {'path': 'dataset/ml-20m/tags.csv',\n",
       "  'name': 'tags.csv',\n",
       "  'extension': '.csv',\n",
       "  'size': 16603996,\n",
       "  'metadata': {'row_count': 465564,\n",
       "   'column_count': 4,\n",
       "   'columns': ['userId', 'movieId', 'tag', 'timestamp'],\n",
       "   'dimensions': None,\n",
       "   'description': 'CSV file with 465564 rows and 4 columns'}},\n",
       " {'path': 'dataset/ml-20m/ratings.csv',\n",
       "  'name': 'ratings.csv',\n",
       "  'extension': '.csv',\n",
       "  'size': 533444411,\n",
       "  'metadata': {'row_count': 20000263,\n",
       "   'column_count': 4,\n",
       "   'columns': ['userId', 'movieId', 'rating', 'timestamp'],\n",
       "   'dimensions': None,\n",
       "   'description': 'CSV file with 20000263 rows and 4 columns'}},\n",
       " {'path': 'dataset/ml-20m/links.csv',\n",
       "  'name': 'links.csv',\n",
       "  'extension': '.csv',\n",
       "  'size': 570090,\n",
       "  'metadata': {'row_count': 27278,\n",
       "   'column_count': 3,\n",
       "   'columns': ['movieId', 'imdbId', 'tmdbId'],\n",
       "   'dimensions': None,\n",
       "   'description': 'CSV file with 27278 rows and 3 columns'}},\n",
       " {'path': 'dataset/ml-20m/genome-tags.csv',\n",
       "  'name': 'genome-tags.csv',\n",
       "  'extension': '.csv',\n",
       "  'size': 18103,\n",
       "  'metadata': {'row_count': 1128,\n",
       "   'column_count': 2,\n",
       "   'columns': ['tagId', 'tag'],\n",
       "   'dimensions': None,\n",
       "   'description': 'CSV file with 1128 rows and 2 columns'}},\n",
       " {'path': 'dataset/ml-1m/users.dat',\n",
       "  'name': 'users.dat',\n",
       "  'extension': '.dat',\n",
       "  'size': 134368,\n",
       "  'metadata': {'row_count': 6040,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'DAT file with 6040 lines'}},\n",
       " {'path': 'dataset/ml-1m/movies.dat',\n",
       "  'name': 'movies.dat',\n",
       "  'extension': '.dat',\n",
       "  'size': 171308,\n",
       "  'metadata': {'row_count': 3883,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'DAT file with 3883 lines'}},\n",
       " {'path': 'dataset/ml-1m/ratings.dat',\n",
       "  'name': 'ratings.dat',\n",
       "  'extension': '.dat',\n",
       "  'size': 24594131,\n",
       "  'metadata': {'row_count': 1000209,\n",
       "   'column_count': None,\n",
       "   'columns': [],\n",
       "   'dimensions': None,\n",
       "   'description': 'DAT file with 1000209 lines'}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender = DatasetRecommender(dataset_dir=\"dataset\")\n",
    "recommender.scan_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_corpus(self):\n",
    "    \"\"\"Analyze the corpus for LLM training-specific metrics.\"\"\"\n",
    "    if not self.file_info:\n",
    "        print(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "        return {}  # Return empty dict instead of None\n",
    "    \n",
    "    print(f\"Found {len(self.file_info)} files to analyze.\")\n",
    "    # Rest of the method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LLMTrainingRecommender(DatasetRecommender):\n",
    "    \"\"\"Extended version of DatasetRecommender specialized for LLM training datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir=\"dataset\"):\n",
    "        super().__init__(dataset_dir)\n",
    "        self.corpus_stats = {}\n",
    "        \n",
    "    def analyze_corpus(self):\n",
    "        \"\"\"Analyze the corpus for LLM training-specific metrics.\"\"\"\n",
    "        if not self.file_info:\n",
    "            print(\"No files scanned yet. Please run scan_datasets() first.\")\n",
    "            return {}  # Return empty dict instead of None\n",
    "        \n",
    "        print(f\"Analyzing {len(self.file_info)} files for LLM training suitability...\")\n",
    "        \n",
    "        # Initialize stats\n",
    "        self.corpus_stats = {\n",
    "            'total_files': len(self.file_info),\n",
    "            'total_size_bytes': sum(file['size'] for file in self.file_info),\n",
    "            'file_types': {},\n",
    "            'estimated_tokens': 0,\n",
    "            'language_distribution': {'English': 0, 'Code': 0, 'Other': 0},\n",
    "            'quality_scores': {'high': 0, 'medium': 0, 'low': 0},\n",
    "            'content_categories': {'Text': 0, 'Code': 0, 'Data': 0, 'Other': 0}\n",
    "        }\n",
    "        \n",
    "        # Count file types\n",
    "        for file in self.file_info:\n",
    "            ext = file['extension']\n",
    "            if ext in self.corpus_stats['file_types']:\n",
    "                self.corpus_stats['file_types'][ext] += 1\n",
    "            else:\n",
    "                self.corpus_stats['file_types'][ext] = 1\n",
    "            \n",
    "            # Very simple token estimation based on file size\n",
    "            tokens = file['size'] // 4  # Simple approximation, 4 bytes per token\n",
    "            self.corpus_stats['estimated_tokens'] += tokens\n",
    "            \n",
    "            # Categorize files (simplified)\n",
    "            if ext in ['.py', '.js', '.java', '.cpp']:\n",
    "                self.corpus_stats['language_distribution']['Code'] += 1\n",
    "                self.corpus_stats['content_categories']['Code'] += 1\n",
    "            elif ext in ['.txt', '.md', '.csv', '.tsv']:\n",
    "                self.corpus_stats['language_distribution']['English'] += 1\n",
    "                self.corpus_stats['content_categories']['Text'] += 1\n",
    "            elif ext in ['.json', '.xlsx', '.xls', '.dat']:\n",
    "                self.corpus_stats['language_distribution']['Other'] += 1\n",
    "                self.corpus_stats['content_categories']['Data'] += 1\n",
    "            else:\n",
    "                self.corpus_stats['language_distribution']['Other'] += 1\n",
    "                self.corpus_stats['content_categories']['Other'] += 1\n",
    "            \n",
    "            # Simple quality assessment\n",
    "            if file['size'] > 10000:\n",
    "                self.corpus_stats['quality_scores']['high'] += 1\n",
    "            elif file['size'] > 1000:\n",
    "                self.corpus_stats['quality_scores']['medium'] += 1\n",
    "            else:\n",
    "                self.corpus_stats['quality_scores']['low'] += 1\n",
    "        \n",
    "        print(f\"Analysis complete. Estimated total tokens: {self.corpus_stats['estimated_tokens']:,}\")\n",
    "        return self.corpus_stats\n",
    "        \n",
    "    # Implementation of other methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.LLMTrainingRecommender'>\n",
      "Scanning dataset for datasets...\n",
      "Found 35 files in the dataset directory.\n",
      "Found 35 files\n",
      "Analyzing 35 files for LLM training suitability...\n",
      "Analysis complete. Estimated total tokens: 604,162,080\n",
      "corpus_stats is None: False\n",
      "Total files: 35\n",
      "Estimated tokens: 604,162,080\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of LLMTrainingRecommender\n",
    "recommender = LLMTrainingRecommender(dataset_dir=\"dataset\")\n",
    "\n",
    "# Check if it's created correctly\n",
    "print(type(recommender))\n",
    "\n",
    "# Scan datasets\n",
    "recommender.scan_datasets()\n",
    "\n",
    "# Check if files were found\n",
    "print(f\"Found {len(recommender.file_info)} files\")\n",
    "\n",
    "# Run analysis\n",
    "corpus_stats = recommender.analyze_corpus()\n",
    "\n",
    "# Check if corpus_stats is None\n",
    "print(f\"corpus_stats is None: {corpus_stats is None}\")\n",
    "\n",
    "# Print some stats\n",
    "if corpus_stats:\n",
    "    print(f\"Total files: {corpus_stats.get('total_files', 'N/A')}\")\n",
    "    print(f\"Estimated tokens: {corpus_stats.get('estimated_tokens', 0):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_mix_recommendation(self, target_tokens=1000000):\n",
    "    \"\"\"Recommend a mix of files to reach the target token count.\"\"\"\n",
    "    if not self.file_info or not hasattr(self, 'corpus_stats'):\n",
    "        print(\"Please run scan_datasets() and analyze_corpus() first.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Generating training mix recommendation for {target_tokens:,} tokens...\")\n",
    "    \n",
    "    # Add estimated_tokens to each file if not already present\n",
    "    for file in self.file_info:\n",
    "        if 'estimated_tokens' not in file:\n",
    "            # Simple estimation: ~4 bytes per token for text files\n",
    "            file['estimated_tokens'] = file['size'] // 4\n",
    "    \n",
    "    # Sort files by size (as a proxy for quality/usefulness)\n",
    "    # In a more sophisticated implementation, you would sort by quality score\n",
    "    size_sorted_files = sorted(self.file_info, key=lambda x: x['size'], reverse=True)\n",
    "    \n",
    "    recommended_files = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Select files until we reach the target token count\n",
    "    for file in size_sorted_files:\n",
    "        tokens = file.get('estimated_tokens', 0)\n",
    "        if tokens == 0:\n",
    "            cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'corpus_stats', 'dataset_dir', 'file_info', 'file_quality_scores', 'min_token_freq', 'sample_size', 'similarity_matrix', 'token_frequency', 'vectorizer']\n"
     ]
    }
   ],
   "source": [
    "print(dir(recommender))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_mix_recommendation(self, target_tokens=1000000):\n",
    "    \"\"\"Recommend a mix of files to reach the target token count.\"\"\"\n",
    "    if not self.file_info or not hasattr(self, 'corpus_stats'):\n",
    "        print(\"Please run scan_datasets() and analyze_corpus() first.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Generating training mix recommendation for {target_tokens:,} tokens...\")\n",
    "    \n",
    "    # Add estimated_tokens to each file if not already present\n",
    "    for file in self.file_info:\n",
    "        if 'estimated_tokens' not in file:\n",
    "            # Simple estimation: ~4 bytes per token for text files\n",
    "            file['estimated_tokens'] = file['size'] // 4\n",
    "    \n",
    "    # Sort files by size (as a proxy for quality/usefulness)\n",
    "    # In a more sophisticated implementation, you would sort by quality score\n",
    "    size_sorted_files = sorted(self.file_info, key=lambda x: x['size'], reverse=True)\n",
    "    \n",
    "    recommended_files = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Select files until we reach the target token count\n",
    "    for file in size_sorted_files:\n",
    "        tokens = file.get('estimated_tokens', 0)\n",
    "        if tokens == 0:\n",
    "            continue  # Skip files with no tokens\n",
    "        \n",
    "        recommended_files.append(file)\n",
    "        current_tokens += tokens\n",
    "        \n",
    "        if current_tokens >= target_tokens:\n",
    "            break\n",
    "    \n",
    "    # If we don't have enough tokens, report what we have\n",
    "    if current_tokens < target_tokens:\n",
    "        print(f\"Warning: Could only find {current_tokens:,} tokens, which is less than the requested {target_tokens:,}.\")\n",
    "    \n",
    "    # Prepare results summary\n",
    "    result = {\n",
    "        'recommended_files': recommended_files,\n",
    "        'total_tokens': current_tokens,\n",
    "        'file_count': len(recommended_files),\n",
    "        'language_distribution': {},\n",
    "        'content_categories': {}\n",
    "    }\n",
    "    \n",
    "    # Count file types in recommendation\n",
    "    for file in recommended_files:\n",
    "        # Simplified language categorization based on file extension\n",
    "        lang = 'Code' if file['extension'] in ['.py', '.js', '.java', '.cpp'] else \\\n",
    "               'English' if file['extension'] in ['.txt', '.md', '.csv', '.tsv'] else 'Other'\n",
    "        \n",
    "        if lang in result['language_distribution']:\n",
    "            result['language_distribution'][lang] += 1\n",
    "        else:\n",
    "            result['language_distribution'][lang] = 1\n",
    "        \n",
    "        # Simplified content categorization\n",
    "        category = 'Code' if file['extension'] in ['.py', '.js', '.java', '.cpp'] else \\\n",
    "                  'Text' if file['extension'] in ['.txt', '.md'] else \\\n",
    "                  'Data' if file['extension'] in ['.csv', '.tsv', '.json', '.xlsx'] else 'Other'\n",
    "        \n",
    "        if category in result['content_categories']:\n",
    "            result['content_categories'][category] += 1\n",
    "        else:\n",
    "            result['content_categories'][category] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LLMTrainingRecommender in module __main__ object:\n",
      "\n",
      "class LLMTrainingRecommender(builtins.object)\n",
      " |  LLMTrainingRecommender(\n",
      " |      dataset_dir='dataset',\n",
      " |      sample_size=5000,\n",
      " |      min_token_freq=5\n",
      " |  )\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, dataset_dir='dataset', sample_size=5000, min_token_freq=5)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(recommender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LLMTrainingRecommender:\n",
    "    def __init__(self, dataset_dir='dataset', sample_size=5000, min_token_freq=5):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.min_token_freq = min_token_freq\n",
    "\n",
    "    def get_training_mix_recommendation(self, target_tokens):\n",
    "        # Implement logic here\n",
    "        return {\"file_count\": 100, \"token_count\": target_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n"
     ]
    }
   ],
   "source": [
    "print(recommender.__module__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_count': 100, 'token_count': 1000000}\n"
     ]
    }
   ],
   "source": [
    "class LLMTrainingRecommender:\n",
    "    def __init__(self, dataset_dir='dataset', sample_size=5000, min_token_freq=5):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.min_token_freq = min_token_freq\n",
    "\n",
    "    def get_training_mix_recommendation(self, target_tokens):\n",
    "        # Example logic (replace with actual implementation)\n",
    "        return {\n",
    "            \"file_count\": 100,  # Example value\n",
    "            \"token_count\": target_tokens\n",
    "        }\n",
    "\n",
    "# Create an instance and call the method\n",
    "recommender = LLMTrainingRecommender()\n",
    "training_mix = recommender.get_training_mix_recommendation(target_tokens=1000000)\n",
    "print(training_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'dataset_dir', 'get_training_mix_recommendation', 'min_token_freq', 'sample_size']\n"
     ]
    }
   ],
   "source": [
    "print(dir(recommender))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_count': 100, 'token_count': 1000000}\n"
     ]
    }
   ],
   "source": [
    "training_mix = recommender.get_training_mix_recommendation(target_tokens=1000000)\n",
    "print(training_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method LLMTrainingRecommender.get_training_mix_recommendation of <__main__.LLMTrainingRecommender object at 0x72ba38766ba0>>\n"
     ]
    }
   ],
   "source": [
    "print(recommender.get_training_mix_recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def get_training_mix_recommendation(self, target_tokens):\n",
      "        # Example logic (replace with actual implementation)\n",
      "        return {\n",
      "            \"file_count\": Integer(100),  # Example value\n",
      "            \"token_count\": target_tokens\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(recommender.get_training_mix_recommendation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_mix_recommendation(self, target_tokens):\n",
    "    return {\n",
    "        \"file_count\": 100,  # Corrected\n",
    "        \"token_count\": target_tokens\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_count': 100, 'token_count': 1000000}\n"
     ]
    }
   ],
   "source": [
    "class LLMTrainingRecommender:\n",
    "    def __init__(self, dataset_dir='dataset', sample_size=5000, min_token_freq=5):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.min_token_freq = min_token_freq\n",
    "\n",
    "    def get_training_mix_recommendation(self, target_tokens):\n",
    "        return {\n",
    "            \"file_count\": 100,  # Fixed issue\n",
    "            \"token_count\": target_tokens\n",
    "        }\n",
    "\n",
    "recommender = LLMTrainingRecommender()\n",
    "print(recommender.get_training_mix_recommendation(target_tokens=1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
